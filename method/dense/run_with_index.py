#!/usr/bin/env python
"""
ä½¿ç”¨é¢„å»ºçš„ç¨ å¯†ç´¢å¼•è¿›è¡Œä»£ç å®šä½æ£€ç´¢

ç”¨æ³•:
    python method/dense/run_with_index.py \
        --index_dir index_data/dense_index_fixed \
        --dataset_path data/Loc-Bench_V1_dataset.jsonl \
        --output_folder outputs/dense_locator_fixed \
        --model_name models/rlretriever \
        --top_k_blocks 50 --top_k_files 10
"""

import argparse
import json
import logging
import os
import os.path as osp
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

# LocAgent ä¾èµ–å·²å¤åˆ¶åˆ° IR-based ä¸‹ï¼Œä¸å†éœ€è¦æ·»åŠ è·¯å¾„

import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

from method.mapping import ASTBasedMapper, GraphBasedMapper
from method.utils import instance_id_to_repo_name as utils_instance_id_to_repo_name, clean_file_path


def instance_id_to_repo_name(instance_id: str) -> str:
    """å°† instance_id è½¬æ¢ä¸º repo_nameï¼ˆå»æ‰ issue ç¼–å·åç¼€ï¼‰"""
    # ä½¿ç”¨ utils ä¸­çš„ç»Ÿä¸€å®ç°
    return utils_instance_id_to_repo_name(instance_id)


def get_problem_text(instance: dict) -> str:
    """ä»å®ä¾‹ä¸­æå–é—®é¢˜æè¿°"""
    for key in ("problem_statement", "issue", "description", "prompt", "text"):
        val = instance.get(key)
        if val:
            return val
    return ""


def embed_texts(
    texts: List[str],
    model: AutoModel,
    tokenizer: AutoTokenizer,
    max_length: int,
    batch_size: int,
    device: torch.device,
) -> torch.Tensor:
    """ç¼–ç æŸ¥è¯¢æ–‡æœ¬"""
    from torch.utils.data import Dataset, DataLoader
    
    class TextDataset(Dataset):
        def __init__(self, items: List[str]):
            self.items = items

        def __len__(self):
            return len(self.items)

        def __getitem__(self, idx: int):
            encoded = tokenizer(
                self.items[idx],
                truncation=True,
                max_length=max_length,
                padding="max_length",
                return_tensors="pt",
            )
            return encoded["input_ids"].squeeze(0), encoded["attention_mask"].squeeze(0)

    ds = TextDataset(texts)
    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)
    model.eval()
    outs: List[torch.Tensor] = []
    with torch.no_grad():
        for input_ids, attn_mask in loader:
            input_ids = input_ids.to(device)
            attn_mask = attn_mask.to(device)
            outputs = model(input_ids=input_ids, attention_mask=attn_mask)
            token_embeddings = outputs[0]
            mask = attn_mask.unsqueeze(-1)
            summed = (token_embeddings * mask).sum(dim=1)
            counts = mask.sum(dim=1).clamp(min=1)
            sent_emb = summed / counts
            sent_emb = torch.nn.functional.normalize(sent_emb, p=2, dim=1)
            outs.append(sent_emb.cpu())
    return torch.cat(outs, dim=0)


def load_index(repo_name: str, index_dir: str) -> Tuple[torch.Tensor, List[dict]]:
    """åŠ è½½é¢„å»ºçš„ç´¢å¼•ï¼Œæ”¯æŒæ ‡å‡†æ ¼å¼å’Œæ›¿æ¢æ ¼å¼"""
    
    # å°è¯•1: æ ‡å‡†æ ¼å¼ï¼ˆå•ä¸‹åˆ’çº¿ï¼Œè½¬æ¢åçš„æ ¼å¼ï¼‰
    repo_index_dir = Path(index_dir) / repo_name
    embeddings_file = repo_index_dir / "embeddings.pt"
    metadata_file = repo_index_dir / "metadata.jsonl"
    
    if embeddings_file.exists() and metadata_file.exists():
        try:
            embeddings = torch.load(embeddings_file, weights_only=True, map_location='cpu')
            metadata = []
            with open(metadata_file, 'r', encoding='utf-8') as f:
                for line in f:
                    metadata.append(json.loads(line))
            
            # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
            if embeddings.shape[0] == len(metadata) and embeddings.shape[0] > 0:
                return embeddings, metadata
        except Exception as e:
            logging.warning(f"Failed to load index for {repo_name}: {e}")
    
    # å°è¯•2: æ›¿æ¢æ ¼å¼ï¼ˆå¤„ç†è¿å­—ç¬¦ç­‰ç‰¹æ®Šæƒ…å†µï¼‰
    # å¦‚æœ repo_name åŒ…å«è¿å­—ç¬¦ï¼Œå°è¯•æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    if '-' in repo_name:
        alt_repo_name = repo_name.replace('-', '_')
        alt_repo_index_dir = Path(index_dir) / alt_repo_name
        alt_embeddings_file = alt_repo_index_dir / "embeddings.pt"
        alt_metadata_file = alt_repo_index_dir / "metadata.jsonl"
        
        if alt_embeddings_file.exists() and alt_metadata_file.exists():
            try:
                embeddings = torch.load(alt_embeddings_file, weights_only=True, map_location='cpu')
                metadata = []
                with open(alt_metadata_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        metadata.append(json.loads(line))
                
                if embeddings.shape[0] == len(metadata) and embeddings.shape[0] > 0:
                    logging.info(f"Found index using alternative naming: {alt_repo_name}")
                    return embeddings, metadata
            except Exception as e:
                logging.warning(f"Failed to load alternative index for {repo_name}: {e}")
    
    logging.warning(f"Index not found for {repo_name} in {index_dir}")
    return None, None


def rank_files(
    block_scores: List[Tuple[int, float]],
    metadata: List[dict],
    top_k_files: int,
    repo_name: str,
) -> List[str]:
    """æ ¹æ®ä»£ç å—åˆ†æ•°èšåˆåˆ°æ–‡ä»¶çº§åˆ«"""
    file_scores: Dict[str, float] = {}
    for block_idx, score in block_scores:
        block_meta = metadata[block_idx]
        file_path = block_meta['file_path']
        # æ¸…ç†æ–‡ä»¶è·¯å¾„ï¼Œä½¿å…¶ä¸ GT æ ¼å¼ä¸€è‡´ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        cleaned_path = clean_file_path(file_path, repo_name)
        file_scores[cleaned_path] = file_scores.get(cleaned_path, 0.0) + float(score)
    
    # æŒ‰åˆ†æ•°æ’åº
    ranked = sorted(file_scores.items(), key=lambda x: x[1], reverse=True)
    return [f for f, _ in ranked[:top_k_files]]


def run(args: argparse.Namespace) -> None:
    # åŠ è½½æ¨¡å‹ï¼ˆåªç”¨äºç¼–ç æŸ¥è¯¢ï¼‰
    if args.force_cpu:
        device = torch.device("cpu")
    elif torch.cuda.is_available():
        device = torch.device(f"cuda:{args.gpu_id}")
        torch.cuda.set_device(args.gpu_id)
    else:
        device = torch.device("cpu")
    
    trust_remote_code = getattr(args, 'trust_remote_code', False)
    # è§„èŒƒåŒ–æ¨¡å‹è·¯å¾„ï¼šå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    model_name = args.model_name
    if not os.path.isabs(model_name):
        # ç›¸å¯¹è·¯å¾„ï¼šå¦‚æœå­˜åœ¨åˆ™è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if os.path.exists(model_name):
            model_name = os.path.abspath(model_name)
    else:
        # ç»å¯¹è·¯å¾„ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•è‡ªåŠ¨ä¿®å¤
        if not os.path.exists(model_name):
            # å°è¯•è‡ªåŠ¨ä¿®å¤å¸¸è§è·¯å¾„é—®é¢˜
            if '/workspace/LocAgent/' in model_name:
                suggested_path = model_name.replace('/workspace/LocAgent/', '/workspace/locbench/LocAgent/')
                if os.path.exists(suggested_path):
                    print(f"âš ï¸  Warning: Model path not found: {model_name}")
                    print(f"ğŸ’¡ Auto-correcting to: {suggested_path}")
                    model_name = suggested_path
                else:
                    print(f"âŒ Error: Model path does not exist: {model_name}")
                    print(f"ğŸ’¡ Suggestion: Did you mean: {suggested_path}?")
                    raise FileNotFoundError(f"Model path not found: {model_name}")
            else:
                print(f"âŒ Error: Model path does not exist: {model_name}")
                raise FileNotFoundError(f"Model path not found: {model_name}")
    print(f"Loading model from {model_name}, trust_remote_code={trust_remote_code}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)
        model = AutoModel.from_pretrained(model_name, trust_remote_code=trust_remote_code).to(device)
    except ValueError as e:
        if "trust_remote_code" in str(e).lower():
            print(f"âŒ Error: Model requires trust_remote_code=True. Please add --trust_remote_code to your command.")
            raise
        else:
            raise
    model.eval()
    print(f"Model loaded on {device}")
    
    # åŠ è½½æ•°æ®é›†
    with open(args.dataset_path, "r") as f:
        instances = [json.loads(line) for line in f]
    if args.eval_n_limit:
        instances = instances[: args.eval_n_limit]
    
    print(f"Loaded {len(instances)} instances")
    
    # æå–æŸ¥è¯¢æ–‡æœ¬
    queries = [get_problem_text(ins) for ins in instances]
    
    # ç¼–ç æ‰€æœ‰æŸ¥è¯¢ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
    print("Encoding queries...")
    query_embeddings = embed_texts(queries, model, tokenizer, args.max_length, args.batch_size, device)
    print(f"Encoded {len(query_embeddings)} queries")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_folder, exist_ok=True)
    output_file = osp.join(args.output_folder, "loc_outputs.jsonl")
    
    # æ ¹æ®mapper_typeé€‰æ‹©æ˜ å°„å™¨
    if args.mapper_type == "graph":
        if not args.graph_index_dir:
            raise ValueError(
                "ä½¿ç”¨Graphæ˜ å°„å™¨æ—¶å¿…é¡»æä¾› --graph_index_dir å‚æ•°ã€‚\n"
                "ç¤ºä¾‹: --mapper_type graph --graph_index_dir index_data/Loc-Bench_V1/graph_index_v2.3"
            )
        mapper = GraphBasedMapper(graph_index_dir=args.graph_index_dir)
        print(f"âœ“ ä½¿ç”¨ Graphæ˜ å°„å™¨ (graph_index_dir: {args.graph_index_dir})")
    else:  # args.mapper_type == "ast"
        if not args.repos_root:
            raise ValueError(
                "ä½¿ç”¨ASTæ˜ å°„å™¨æ—¶å¿…é¡»æä¾› --repos_root å‚æ•°ã€‚\n"
                "ç¤ºä¾‹: --mapper_type ast --repos_root /workspace/locbench/repos/locbench_repos"
            )
        mapper = ASTBasedMapper(repos_root=args.repos_root)
        print(f"âœ“ ä½¿ç”¨ ASTæ˜ å°„å™¨ (repos_root: {args.repos_root})")
    
    # ç¼“å­˜ç´¢å¼•ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
    index_cache: Dict[str, Tuple[torch.Tensor, List[dict]]] = {}
    
    def get_cached_index(repo_name: str):
        if repo_name not in index_cache:
            embeddings, metadata = load_index(repo_name, args.index_dir)
            # ç´¢å¼•ä¿ç•™åœ¨ CPUï¼Œé¿å… GPU å†…å­˜ä¸è¶³
            index_cache[repo_name] = (embeddings, metadata)
        return index_cache[repo_name]
    
    # å¤„ç†æ¯ä¸ªå®ä¾‹
    print("Running retrieval...")
    index_found = 0
    index_missing = 0
    missing_repos = []
    
    with open(output_file, "w") as fout, torch.no_grad():
        for ins, query_emb in tqdm(zip(instances, query_embeddings), total=len(instances), desc="Retrieving"):
            instance_id = ins.get("instance_id", "")
            repo_name = instance_id_to_repo_name(instance_id)
            
            # åŠ è½½ç´¢å¼•ï¼ˆåœ¨ CPU ä¸Šï¼‰
            embeddings, metadata = get_cached_index(repo_name)
            
            if embeddings is None or metadata is None:
                # ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ
                index_missing += 1
                missing_repos.append(repo_name)
                record = {
                    "instance_id": instance_id,
                    "found_files": [],
                    "found_modules": [],
                    "found_entities": [],
                    "raw_output_loc": [],
                }
                fout.write(json.dumps(record) + "\n")
                continue
            
            index_found += 1
            
            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä¸´æ—¶ç§»åˆ° GPUï¼‰
            query_emb_gpu = query_emb.to(device)
            embeddings_gpu = embeddings.to(device)  # ä¸´æ—¶ç§»åˆ° GPU
            scores = torch.matmul(query_emb_gpu.unsqueeze(0), embeddings_gpu.t()).squeeze(0)  # (num_blocks,)
            scores = scores.cpu()  # ç§»å› CPU ä»¥ä¾¿åç»­å¤„ç†
            
            # è·å– Top-K ä»£ç å—
            topk = min(args.top_k_blocks, scores.numel())
            if topk == 0:
                found_files = []
                found_modules = []
                found_entities = []
            else:
                topk_scores, topk_idx = torch.topk(scores, k=topk)
                block_scores = list(zip(topk_idx.tolist(), topk_scores.tolist()))
                found_files = rank_files(block_scores, metadata, args.top_k_files, repo_name)
                
                # æ˜ å°„ä»£ç å—åˆ°å‡½æ•°/æ¨¡å—
                # æ¸…ç† top_blocks ä¸­çš„ file_pathï¼Œä½¿å…¶ä¸ GT æ ¼å¼ä¸€è‡´
                top_blocks = []
                for idx, _ in block_scores:
                    block = metadata[idx].copy()  # å¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸå§‹ metadata
                    original_path = block.get('file_path', '')
                    if original_path:
                        block['file_path'] = clean_file_path(original_path, repo_name)
                    top_blocks.append(block)
                
                found_modules, found_entities = mapper.map_blocks_to_entities(
                    blocks=top_blocks,
                    instance_id=instance_id,
                    top_k_modules=args.top_k_modules,
                    top_k_entities=args.top_k_entities,
                )
            
            # ä¿å­˜ç»“æœ
            record = {
                "instance_id": instance_id,
                "found_files": found_files,
                "found_modules": found_modules,
                "found_entities": found_entities,
                "raw_output_loc": [],
            }
            fout.write(json.dumps(record) + "\n")
    
    # è¾“å‡ºç´¢å¼•æŸ¥æ‰¾ç»Ÿè®¡
    print(f"\nIndex Statistics:")
    print(f"  Found: {index_found}/{len(instances)}")
    print(f"  Missing: {index_missing}/{len(instances)}")
    if missing_repos:
        unique_missing = list(set(missing_repos))[:10]
        print(f"  Missing repos (sample): {unique_missing}")
    
    print(f"\nResults saved to {output_file}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="ä½¿ç”¨é¢„å»ºçš„ç¨ å¯†ç´¢å¼•è¿›è¡Œä»£ç å®šä½æ£€ç´¢")
    parser.add_argument("--index_dir", type=str, required=True,
                        help="é¢„å»ºç´¢å¼•ç›®å½•ï¼ˆå¦‚ index_data/dense_index_fixedï¼‰")
    parser.add_argument("--dataset_path", type=str, required=True,
                        help="æ•°æ®é›† JSONL æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_folder", type=str, required=True,
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model_name", type=str, default="models/rlretriever",
                        help="æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºç¼–ç æŸ¥è¯¢ï¼‰")
    parser.add_argument("--trust_remote_code", action="store_true",
                        help="å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç ï¼ˆCodeRankEmbed ç­‰æ¨¡å‹éœ€è¦ï¼‰")
    parser.add_argument("--gpu_id", type=int, default=0,
                        help="æŒ‡å®šä½¿ç”¨çš„ GPU IDï¼ˆé»˜è®¤ 0ï¼‰")
    parser.add_argument("--max_length", type=int, default=512,
                        help="æœ€å¤§ token é•¿åº¦")
    parser.add_argument("--batch_size", type=int, default=8,
                        help="æ‰¹é‡å¤§å°")
    parser.add_argument("--top_k_blocks", type=int, default=50,
                        help="Top-K ä»£ç å—")
    parser.add_argument("--top_k_files", type=int, default=15,
                        help="Top-K æ–‡ä»¶")
    parser.add_argument("--top_k_modules", type=int, default=15,
                        help="è¿”å›çš„æ¨¡å—æ•°é‡")
    parser.add_argument("--top_k_entities", type=int, default=15,
                        help="è¿”å›çš„å®ä½“æ•°é‡")
    parser.add_argument(
        "--mapper_type",
        type=str,
        choices=["ast", "graph"],
        default="ast",
        help="æ˜ å°„å™¨ç±»å‹: 'ast' (ASTè§£æ, é»˜è®¤) æˆ– 'graph' (Graphç´¢å¼•+span_ids)"
    )
    parser.add_argument(
        "--graph_index_dir",
        type=str,
        default=None,
        help="Graphç´¢å¼•ç›®å½•ï¼ˆä½¿ç”¨ --mapper_type graph æ—¶å¿…éœ€ï¼‰"
    )
    parser.add_argument(
        "--repos_root",
        type=str,
        default="/workspace/locbench/repos/locbench_repos",
        help="ä»£ç ä»“åº“æ ¹ç›®å½•ï¼ˆä½¿ç”¨ --mapper_type ast æ—¶å¿…éœ€ï¼Œé»˜è®¤: /workspace/locbench/repos/locbench_reposï¼‰"
    )
    parser.add_argument("--eval_n_limit", type=int, default=0,
                        help="é™åˆ¶å¤„ç†çš„å®ä¾‹æ•°é‡ï¼ˆ0 è¡¨ç¤ºå…¨éƒ¨ï¼‰")
    parser.add_argument("--force_cpu", action="store_true",
                        help="å¼ºåˆ¶ä½¿ç”¨ CPU")
    return parser.parse_args()


if __name__ == "__main__":
    run(parse_args())

