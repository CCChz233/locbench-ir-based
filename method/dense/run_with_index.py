#!/usr/bin/env python
"""
CodeRankEmbed ä¸“ç”¨ç´¢å¼•è¯„ä¼°å·¥å…·
ä½¿ç”¨ transformers AutoModel åç«¯
"""

import argparse
import json
import logging
import os
import os.path as osp
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple

import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

from method.mapping import ASTBasedMapper, GraphBasedMapper
from method.utils import instance_id_to_repo_name as utils_instance_id_to_repo_name, clean_file_path


def instance_id_to_repo_name(instance_id: str) -> str:
    """å°† instance_id è½¬æ¢ä¸º repo_nameï¼ˆå»æ‰ issue ç¼–å·åç¼€ï¼‰"""
    # ä½¿ç”¨ utils ä¸­çš„ç»Ÿä¸€å®ç°
    return utils_instance_id_to_repo_name(instance_id)


def get_problem_text(instance: dict) -> str:
    """ä»å®ä¾‹ä¸­æå–é—®é¢˜æè¿°"""
    for key in ("problem_statement", "issue", "description", "prompt", "text"):
        val = instance.get(key)
        if val:
            return val
    return ""


def embed_texts(
    texts: List[str],
    model: AutoModel,  # æ˜ç¡®ç±»å‹
    tokenizer: AutoTokenizer,
    max_length: int,
    batch_size: int,
    device: torch.device,
) -> torch.Tensor:
    """CodeRankEmbed ä¸“ç”¨çš„æ–‡æœ¬ embedding å‡½æ•°"""
    from torch.utils.data import Dataset, DataLoader

    class TextDataset(Dataset):
        def __init__(self, items: List[str]):
            self.items = items

        def __len__(self):
            return len(self.items)

        def __getitem__(self, idx: int):
            encoded = tokenizer(
                self.items[idx],
                truncation=True,
                max_length=max_length,
                padding="max_length",
                return_tensors="pt",
            )
            return encoded["input_ids"].squeeze(0), encoded["attention_mask"].squeeze(0)

    ds = TextDataset(texts)
    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)
    model.eval()
    outs: List[torch.Tensor] = []
    with torch.no_grad():
        for input_ids, attn_mask in loader:
            input_ids = input_ids.to(device)
            attn_mask = attn_mask.to(device)
            # CodeRankEmbed ä½¿ç”¨ AutoModel
            outputs = model(input_ids=input_ids, attention_mask=attn_mask)
            token_embeddings = outputs[0]
            sent_emb = token_embeddings[:, 0]  # [CLS] token
            sent_emb = torch.nn.functional.normalize(sent_emb, p=2, dim=1)
            outs.append(sent_emb.cpu())
    return torch.cat(outs, dim=0)


def rank_files(
    block_scores: List[Tuple[int, float]],
    metadata: List[dict],
    top_k_files: int,
    repo_name: str,
) -> List[str]:
    """æ ¹æ®ä»£ç å—åˆ†æ•°èšåˆåˆ°æ–‡ä»¶çº§åˆ«"""
    file_scores: Dict[str, float] = {}
    for block_idx, score in block_scores:
        block_meta = metadata[block_idx]
        file_path = block_meta['file_path']
        # æ¸…ç†æ–‡ä»¶è·¯å¾„ï¼Œä½¿å…¶ä¸ GT æ ¼å¼ä¸€è‡´ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        cleaned_path = clean_file_path(file_path, repo_name)
        file_scores[cleaned_path] = file_scores.get(cleaned_path, 0.0) + float(score)

    # æŒ‰åˆ†æ•°æ’åº
    ranked = sorted(file_scores.items(), key=lambda x: x[1], reverse=True)
    return [f for f, _ in ranked[:top_k_files]]


def load_index(repo_name: str, index_dir: str) -> Tuple[torch.Tensor, List[dict]]:
    """åŠ è½½é¢„å»ºçš„ç´¢å¼•"""
    index_path = Path(index_dir) / repo_name / "embeddings.pt"
    metadata_path = Path(index_dir) / repo_name / "metadata.jsonl"

    if not index_path.exists():
        raise FileNotFoundError(f"Index file not found: {index_path}")
    if not metadata_path.exists():
        raise FileNotFoundError(f"Metadata file not found: {metadata_path}")

    # åŠ è½½embeddings
    embeddings = torch.load(index_path, map_location='cpu')

    # åŠ è½½metadata
    metadata = []
    with metadata_path.open('r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                metadata.append(json.loads(line))

    return embeddings, metadata


def load_model(
    model_name: str,
    device: torch.device,
    trust_remote_code: bool = False,
) -> Tuple[AutoModel, AutoTokenizer]:
    """åŠ è½½ CodeRankEmbed æ¨¡å‹ï¼ˆå›ºå®šä½¿ç”¨ AutoModelï¼‰"""
    print(f"Loading CodeRankEmbed model from {model_name}")

    if not os.path.exists(model_name):
        print(f"âŒ Error: Model path does not exist: {model_name}")
        raise FileNotFoundError(f"Model path not found: {model_name}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=trust_remote_code
        )
        model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=trust_remote_code
        ).to(device)
        model.eval()
        print(f"âœ… Model loaded successfully")
        return model, tokenizer
    except ValueError as e:
        if "trust_remote_code" in str(e).lower():
            print(f"âŒ Error: Model requires trust_remote_code=True")
            raise
        else:
            raise


def main():
    parser = argparse.ArgumentParser(
        description="CodeRankEmbed ä¸“ç”¨ç´¢å¼•è¯„ä¼°å·¥å…·"
    )

    # æ•°æ®å‚æ•°
    parser.add_argument("--dataset_path", type=str, required=True,
                       help="Path to dataset JSONL file")
    parser.add_argument("--index_dir", type=str, required=True,
                       help="Directory containing pre-built indexes")
    parser.add_argument("--output_folder", type=str, required=True,
                       help="Output directory for results")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_name", type=str, required=True,
                       help="CodeRankEmbed model path")
    parser.add_argument("--trust_remote_code", action="store_true",
                       help="Allow execution of model repository code")

    # æ£€ç´¢å‚æ•°
    parser.add_argument("--top_k_blocks", type=int, default=50,
                       help="Number of top blocks to retrieve")
    parser.add_argument("--top_k_files", type=int, default=10,
                       help="Number of top files to retrieve")
    parser.add_argument("--top_k_modules", type=int, default=10,
                       help="Number of top modules to retrieve")
    parser.add_argument("--top_k_entities", type=int, default=50,
                       help="Number of top entities to retrieve")
    parser.add_argument("--max_length", type=int, default=4096,
                       help="Maximum sequence length")
    parser.add_argument("--batch_size", type=int, default=8,
                       help="Batch size for embedding")

    # æ˜ å°„å‚æ•°
    parser.add_argument("--mapper_type", type=str, default="ast",
                       choices=["ast", "graph"],
                       help="Type of mapper to use")
    parser.add_argument("--repos_root", type=str,
                       default="/workspace/locbench/repos/locbench_repos",
                       help="Root directory of repositories")

    # å…¶ä»–å‚æ•°
    parser.add_argument("--gpu_id", type=int, default=0,
                       help="GPU ID to use")
    parser.add_argument("--force_cpu", action="store_true",
                       help="Force CPU usage")

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    if args.force_cpu:
        device = torch.device('cpu')
    else:
        device = torch.device(f'cuda:{args.gpu_id}')

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_folder)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    try:
        model, tokenizer = load_model(args.model_name, device, args.trust_remote_code)
    except Exception as e:
        print(f"Failed to load model: {e}")
        return

    # åŠ è½½æ•°æ®é›†
    print(f"Loading dataset from {args.dataset_path}")
    dataset = []
    with open(args.dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                dataset.append(json.loads(line))

    print(f"Loaded {len(dataset)} instances")

    # åˆå§‹åŒ–æ˜ å°„å™¨
    if args.mapper_type == "ast":
        mapper = ASTBasedMapper(args.repos_root)
    else:
        mapper = GraphBasedMapper(args.repos_root)

    results = []

    # ç¼“å­˜ç´¢å¼•ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
    index_cache: Dict[str, Tuple[torch.Tensor, List[dict]]] = {}

    def get_cached_index(repo_name: str):
        if repo_name not in index_cache:
            embeddings, metadata = load_index(repo_name, args.index_dir)
            # ç´¢å¼•ä¿ç•™åœ¨ CPUï¼Œé¿å… GPU å†…å­˜ä¸è¶³
            index_cache[repo_name] = (embeddings, metadata)
        return index_cache[repo_name]

    # å¤„ç†ç»Ÿè®¡
    index_found = 0
    index_missing = 0
    missing_repos = []

    for instance in tqdm(dataset, desc="Processing instances"):
        instance_id = instance.get("instance_id", "")
        repo_name = instance_id_to_repo_name(instance_id)

        try:
            # åŠ è½½ç´¢å¼•ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            embeddings, metadata = get_cached_index(repo_name)

            if embeddings is None or metadata is None:
                # ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ
                index_missing += 1
                missing_repos.append(repo_name)
                results.append({
                    "instance_id": instance_id,
                    "found_files": [],
                    "found_modules": [],
                    "found_entities": [],
                    "raw_output_loc": []
                })
                continue

            index_found += 1

            # è·å–é—®é¢˜æ–‡æœ¬
            query_text = get_problem_text(instance)
            if not query_text:
                print(f"Warning: No query text found for instance {instance_id}")
                continue

            # ç¼–ç æŸ¥è¯¢
            query_embedding = embed_texts([query_text], model, tokenizer, args.max_length, 1, device)[0]

            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä¸´æ—¶ç§»åˆ° GPUï¼‰
            query_emb_gpu = query_embedding.to(device)
            embeddings_gpu = embeddings.to(device)  # ä¸´æ—¶ç§»åˆ° GPU
            similarities = torch.matmul(query_emb_gpu.unsqueeze(0), embeddings_gpu.t()).squeeze(0)  # (num_blocks,)
            similarities = similarities.cpu()  # ç§»å› CPU ä»¥ä¾¿åç»­å¤„ç†

            # è·å–top-kå—ï¼ˆç¡®ä¿ä¸è¶…è¿‡å¯ç”¨å—æ•°ï¼‰
            k = min(args.top_k_blocks, len(similarities))
            if k == 0:
                found_files = []
                found_modules = []
                found_entities = []
            else:
                top_k_values, top_k_indices = torch.topk(similarities, k)
                block_scores = list(zip(top_k_indices.tolist(), top_k_values.tolist()))

                # è·å–æ–‡ä»¶çº§åˆ«ç»“æœ
                found_files = rank_files(block_scores, metadata, args.top_k_files, repo_name)

                # æ˜ å°„ä»£ç å—åˆ°å‡½æ•°/æ¨¡å—
                # æ¸…ç† top_blocks ä¸­çš„ file_pathï¼Œä½¿å…¶ä¸ GT æ ¼å¼ä¸€è‡´
                top_blocks = []
                for idx, _ in block_scores:
                    block = metadata[idx].copy()  # å¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸå§‹ metadata
                    original_path = block.get('file_path', '')
                    if original_path:
                        block['file_path'] = clean_file_path(original_path, repo_name)
                    top_blocks.append(block)

                found_modules, found_entities = mapper.map_blocks_to_entities(
                    blocks=top_blocks,
                    instance_id=instance_id,
                    top_k_modules=args.top_k_modules,
                    top_k_entities=args.top_k_entities,
                )

            # ä¿å­˜ç»“æœ
            results.append({
                "instance_id": instance_id,
                "found_files": found_files,
                "found_modules": found_modules,
                "found_entities": found_entities,
                "raw_output_loc": []
            })

        except Exception as e:
            print(f"Error processing instance {instance_id}: {e}")
            results.append({
                "instance_id": instance_id,
                "error": str(e),
                "top_files": []
            })

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_instances = len(results)
    successful_instances = sum(1 for r in results if r.get('top_files') and len(r['top_files']) > 0)
    failed_instances = sum(1 for r in results if 'error' in r)

    if successful_instances > 0:
        avg_files = sum(len(r.get('top_files', [])) for r in results) / successful_instances
    else:
        avg_files = 0

    # ä¿å­˜ç»“æœ
    output_path = output_dir / "loc_outputs.jsonl"
    with output_path.open('w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')

    print(f"Results saved to {output_path}")
    print(f"ğŸ“Š Retrieval Summary:")
    print(f"   Total instances: {total_instances}")
    print(f"   Successful: {successful_instances} ({successful_instances/total_instances:.1%})")
    print(f"   Failed: {failed_instances}")
    print(f"   Average files per instance: {avg_files:.1f}")

    # è¾“å‡ºç´¢å¼•æŸ¥æ‰¾ç»Ÿè®¡
    print(f"\nIndex Statistics:")
    print(f"  Found: {index_found}/{len(dataset)}")
    print(f"  Missing: {index_missing}/{len(dataset)}")
    if missing_repos:
        unique_missing = list(set(missing_repos))[:10]
        print(f"  Missing repos (sample): {unique_missing}")

    if successful_instances > 0:
        print(f"\nâœ… Retrieval completed successfully!")
    else:
        print(f"\nâŒ No successful retrievals!")


if __name__ == "__main__":
    main()